# Repository Summary - Dark Data Docling Extractors

**Created**: November 2, 2025
**Purpose**: Lightweight extraction repository for Claude Code efficiency

---

## ğŸ“¦ What This Repository Contains

This is a **focused extraction repository** containing only Docling-related tools and domain processors.

### Total Size: **70 MB**
- `domains/` - 37 MB (domain processors)
- `docling_layout/` - 34 MB (Docling infrastructure + outputs)

---

## ğŸ“ Structure

```
dark-data-docling-extractors/
â”‚
â”œâ”€â”€ docling_layout/              # 34 MB - Docling extraction tools
â”‚   â”œâ”€â”€ eaf_patch/               # EAF monkey patch engine
â”‚   â”‚   â”œâ”€â”€ core/                # Patch engine, detectors, post-processors
â”‚   â”‚   â”‚   â”œâ”€â”€ eaf_patch_engine.py          # Main patch engine
â”‚   â”‚   â”‚   â”œâ”€â”€ eaf_title_detector.py        # Title detection
â”‚   â”‚   â”‚   â”œâ”€â”€ company_name_detector.py     # Company names
â”‚   â”‚   â”‚   â”œâ”€â”€ power_line_classifier.py     # Power line detection
â”‚   â”‚   â”‚   â””â”€â”€ post_processors/             # Document-level fixes
â”‚   â”‚   â”‚       â”œâ”€â”€ zona_fix.py              # Zona classification fix
â”‚   â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scripts/             # Testing and development scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ METHODOLOGY/             # Complete documentation (15+ files)
â”‚   â”‚   â”œâ”€â”€ RESUMEN_METODOLOGIA.md           # â­ Start here
â”‚   â”‚   â”œâ”€â”€ EAF_PATCH_ARCHITECTURE.md        # Monkey patch architecture
â”‚   â”‚   â”œâ”€â”€ UNIVERSAL_DOCLING_METHODOLOGY.md # Complete guide
â”‚   â”‚   â”œâ”€â”€ QUICK_START_GUIDE.md             # 1-page reference
â”‚   â”‚   â””â”€â”€ ... (10+ more docs)
â”‚   â”‚
â”‚   â”œâ”€â”€ EXTRACT_ANY_CHAPTER.py   # â­ Universal extraction script
â”‚   â”œâ”€â”€ capitulo_XX/             # Chapter outputs (gitignored PDFs/JSONs)
â”‚   â””â”€â”€ README.md                # Docling documentation
â”‚
â”œâ”€â”€ domains/                     # 37 MB - Domain processors
â”‚   â”œâ”€â”€ operaciones/
â”‚   â”‚   â”œâ”€â”€ anexos_eaf/          # EAF annexes processing
â”‚   â”‚   â”‚   â”œâ”€â”€ chapters/        # ANEXO 1, 2, informe_diario
â”‚   â”‚   â”‚   â””â”€â”€ shared/          # Cross-chapter utilities
â”‚   â”‚   â”œâ”€â”€ eaf/                 # Individual EAF reports
â”‚   â”‚   â”‚   â”œâ”€â”€ chapters/        # 11 chapters (capitulo_01-11)
â”‚   â”‚   â”‚   â””â”€â”€ shared/          # Cross-chapter utilities
â”‚   â”‚   â””â”€â”€ shared/              # Domain-wide utilities
â”‚   â”œâ”€â”€ mercados/                # Energy markets (planned)
â”‚   â”œâ”€â”€ legal/                   # Legal compliance (planned)
â”‚   â””â”€â”€ planificacion/           # Planning (planned)
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies (clean, project-only)
â”œâ”€â”€ .gitignore                   # Ignore PDFs, outputs, venv
â”œâ”€â”€ README.md                    # Main documentation
â””â”€â”€ REPOSITORY_SUMMARY.md        # This file
```

---

## ğŸ¯ Why This Repository Was Created

### Problem: Monorepo Too Large for Claude Code
The original `Proyecto Dark Data CEN` contained:
- Extraction tools (40 MB code)
- MCP servers (2 MB code)
- Database infrastructure
- Web dashboard
- AI platform
- **Total context: 50,000+ tokens for Claude**

### Solution: Split into 2 Repositories

**This repo** (`dark-data-docling-extractors`):
- âœ… Only extraction code
- âœ… Only Docling tools
- âœ… Only domain processors
- âœ… 70 MB total (lightweight)
- âœ… ~15,000 tokens context for Claude
- âœ… 90% of development time

**Other repo** (`dark-data-platform` - not created yet):
- MCP servers
- Database infrastructure
- Web dashboard
- AI platform
- ~2 MB (infrastructure only)
- ~10,000 tokens context
- 10% of development time

---

## âœ… What You Can Do Here

### Extract Any Chapter (1-11)
```bash
cd docling_layout
python3 EXTRACT_ANY_CHAPTER.py 6   # Extract Chapter 6
python3 EXTRACT_ANY_CHAPTER.py 7   # Extract Chapter 7
```

### Process All Chapters in Batch
```bash
for i in {1..11}; do
  python3 EXTRACT_ANY_CHAPTER.py $i
done
```

### Develop New Processors
```bash
cd domains/operaciones/eaf/chapters/capitulo_XX/processors
# Create new processor for specific chapter
```

### Work with Claude Code
```bash
cd dark-data-docling-extractors
claude "help me extract chapter 8 with monkey patch"
# Claude sees only extraction context (fast, focused)
```

---

## ğŸš« What You CANNOT Do Here

âŒ Run MCP servers â†’ See `dark-data-platform` repo (when created)
âŒ Access database â†’ See `dark-data-platform` repo
âŒ Use web dashboard â†’ See `dark-data-platform` repo
âŒ Query with AI â†’ See `dark-data-platform` repo

This repo is **extraction-only** by design.

---

## ğŸ“Š Key Features

### 1. EAF Monkey Patch (Automatic)
- Detects missing titles automatically
- Cross-page list detection
- Company name detection
- Power line classification
- No manual intervention required

### 2. Universal Extraction Script
- One script for ALL chapters (1-11)
- Only change: chapter number
- Automatic outputs:
  - JSON: `capitulo_XX/outputs/layout_WITH_PATCH.json`
  - PDF: `capitulo_XX/outputs/chapterXX_WITH_PATCH_annotated.pdf`

### 3. Complete Documentation
15+ methodology documents in `METHODOLOGY/`:
- Architecture guides
- Configuration references
- Quick start guides
- Troubleshooting docs

### 4. GPU Optimization
- Lightweight mode: 1.3 GB VRAM (fits 4GB GPU)
- Standard mode: 4.2 GB VRAM (requires 6GB+ GPU)
- CPU fallback: 400 MB RAM (slow but works)

---

## ğŸ”§ Dependencies (Clean)

See `requirements.txt` for complete list. Key dependencies:

**Core:**
- `docling==2.17.0` - IBM Docling Granite-258M
- `PyMuPDF==1.25.1` - PDF text extraction
- `torch==2.5.1` - AI model inference

**Optional:**
- `anthropic==0.40.0` - Claude API (validation)
- `pandas==2.2.3` - Data processing

**Total:** ~46 packages (clean, project-only dependencies)

---

## ğŸ“ˆ Comparison: Before vs After

| Aspect | Monorepo (Before) | This Repo (After) |
|--------|------------------|-------------------|
| **Total size** | 230 MB | 70 MB (70% reduction) |
| **Code size** | 40 MB extraction + 2 MB platform | 70 MB extraction only |
| **Claude context** | 50,000+ tokens | ~15,000 tokens (70% reduction) |
| **Search speed** | Slow (searches all code) | Fast (searches extraction only) |
| **Focus** | Confusing (sees everything) | Clear (sees extraction only) |
| **Git operations** | Slow | Fast |
| **Claude sessions** | "What's this MCP server?" | "Extract chapter 8" âœ… |

---

## ğŸš€ Next Steps

### For This Repository
1. âœ… Repository created with Docling tools + domains
2. âœ… Clean requirements.txt generated
3. âœ… Git initialized
4. â³ Push to GitHub (when ready)
5. â³ Add CI/CD for automated testing

### For Platform Repository (Future)
1. â³ Create `dark-data-platform` repository
2. â³ Copy MCP servers, database, web UI
3. â³ Connect via JSON export/import
4. â³ Push to GitHub

---

## ğŸ“ Usage Tips

### Working with Claude Code
```bash
# In this repo:
cd dark-data-docling-extractors
claude "fix title detection in Chapter 6"
# Claude sees only extraction code âœ…

# In platform repo (future):
cd dark-data-platform
claude "add MCP tool to query chapter titles"
# Claude sees only platform code âœ…
```

### Connecting the Two Repos
```bash
# 1. Extract in this repo
cd dark-data-docling-extractors/docling_layout
python3 EXTRACT_ANY_CHAPTER.py 6

# 2. Copy JSON to platform (manual for now)
cp capitulo_06/outputs/layout_WITH_PATCH.json \
   ../../dark-data-platform/data/universal_json/

# 3. Ingest in platform repo
cd ../../dark-data-platform
make ingest-data
```

---

## ğŸ¯ Success Metrics

This repository is successful if:
- âœ… Claude Code sessions are fast and focused
- âœ… Extraction development is streamlined
- âœ… No confusion about "what goes where"
- âœ… Search/grep operations are instant
- âœ… Git operations are quick
- âœ… New developers understand structure immediately

---

**Repository optimized for Claude Code efficiency and extraction development** ğŸš€
